import numpy as np
import copy

class ENV :
    def __init__(self):
        self.A_EDGE               = 0.3
        self.ACTION_SPACE         = np.array([0.1,0.2])
        self.DEPARTURE_CLOUD      = np.array([1,1]) * 0.3
        self.CAPACITY_EDGE        = np.array([1,1,1,1])
        self.CAPACITY_CLOUD       = np.array([1,1])
        self.N_AGENTS             = 4
        self.N_CLOUD              = 2
        self.T_MAX                = 30
        self.time                 = 0
        self.q_prev_edge          = np.array([0.8,0.8,0.2,0.2])
        self.q_curr_edge          = np.array([0.8,0.8,0.2,0.2])
        self.q_curr_cloud         = np.array([0.5,0.5])
        self.action_dim           = 4
        self.state_dim            = 4
        
    def get_state(self):
        return np.array([ np.hstack([self.q_prev_edge[i],self.q_curr_edge[i],self.q_curr_cloud]) for i in range(self.N_AGENTS)])
    
    def step(self, actions):
        self.q_prev_edge     = np.copy(self.q_curr_edge)
        self.indicator_edge = np.zeros(self.N_AGENTS)
        self.actions        = np.zeros(self.N_AGENTS)
        
        for n in range(self.N_AGENTS):
            if actions[n] == 0:
                self.indicator_edge[n] = 0
                self.actions[n]        = self.ACTION_SPACE[0]
            elif actions[n] == 1:  
                self.indicator_edge[n] = 0
                self.actions[n] = self.ACTION_SPACE[1]
            elif actions[n] ==  2:
                self.indicator_edge[n] = 1
                self.actions[n] = self.ACTION_SPACE[0]
            elif actions[n] == 3:  
                self.indicator_edge[n] = 1
                self.actions[n] = self.ACTION_SPACE[1]
        
        
        self.arrival_edge    = np.random.rand( self.N_AGENTS)* self.A_EDGE
        self.q_curr_edge     = self.q_curr_edge +  self.arrival_edge  - self.actions
        self.q_loss_edge     = ( self.q_curr_edge >  self.CAPACITY_EDGE) * np.abs(self.q_curr_edge- self.CAPACITY_EDGE)
        self.q_stall_edge    = (self.q_curr_edge < 0) * np.abs(self.q_curr_edge)
        self.q_curr_edge     = np.clip(self.q_curr_edge,0,1)

        self.actions         = self.actions - self.q_stall_edge

        self.arrival_cloud   = np.array([ ((self.indicator_edge== i) * self.actions).sum() for i in range(self.N_CLOUD)])
        self.q_curr_cloud    = self.q_curr_cloud + self.arrival_cloud - self.DEPARTURE_CLOUD
        self.q_loss_cloud    = (self.q_curr_cloud > self.CAPACITY_CLOUD) * np.abs(self.q_curr_cloud- self.CAPACITY_CLOUD)
        self.q_stall_cloud   = (self.q_curr_cloud<0) * np.abs(self.q_curr_cloud)
        self.departure_cloud = self.DEPARTURE_CLOUD - self.q_stall_cloud
        self.q_curr_cloud    = np.clip(self.q_curr_cloud, 0, 1)
        
        
        comm_r     =  1*self.q_loss_cloud.sum()  +  4*self.q_stall_cloud.sum()
        rewards      = -comm_r
        
        next_state = self.get_state()
        self.time += 1
        
        if self.time >=self.T_MAX:
            done = True
        else:
            done = False
            
        return rewards,  next_state, done

    def reset(self):
        self.q_prev_edge     = np.array([0.8,0.8,0.2,0.2])
        self.q_curr_edge     = np.array([0.8,0.8,0.2,0.2])
        self.q_curr_cloud    = np.array([0.5,0.5])
        return self.get_state()



import gym
import numpy as np
import copy
import torch
import torch.nn.functional as F
import torch.optim as optim
import argparse
import torch.nn as nn
import torchquantum as tq
import torchquantum.functional as tqf
from torch.optim.lr_scheduler import CosineAnnealingLR
from  torch.distributions import Categorical
from Environment import ENV
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime

class QActor(tq.QuantumModule):
    class QLayer(tq.QuantumModule):
        def __init__(self):
            super().__init__()
            self.n_wires = 4
            self.random_layer = tq.RandomLayer(n_ops=50,
                                               wires=list(range(self.n_wires)))
            # gates with trainable parameters
            self.rx0 = tq.RX(has_params=True, trainable=True)
            self.ry0 = tq.RY(has_params=True, trainable=True)
            self.rz0 = tq.RZ(has_params=True, trainable=True)
            self.crx0 = tq.CRX(has_params=True, trainable=True)

        @tq.static_support
        def forward(self, q_device: tq.QuantumDevice):
            """
            1. To convert tq QuantumModule to qiskit or run in the static
            model, need to:
                (1) add @tq.static_support before the forward
                (2) make sure to add
                    static=self.static_mode and
                    parent_graph=self.graph
                    to all the tqf functions, such as tqf.hadamard below
            """
            self.q_device = q_device

            self.random_layer(self.q_device)

            # some trainable gates (instantiated ahead of time)
            self.rx0(self.q_device, wires=0)
            self.ry0(self.q_device, wires=1)
            self.rz0(self.q_device, wires=3)
            self.crx0(self.q_device, wires=[0, 2])
            """
            # add some more non-parameterized gates (add on-the-fly)
            tqf.hadamard(self.q_device, wires=3, static=self.static_mode,
                         parent_graph=self.graph)
            tqf.sx(self.q_device, wires=2, static=self.static_mode,
                   parent_graph=self.graph)
            tqf.cnot(self.q_device, wires=[3, 0], static=self.static_mode,
                     parent_graph=self.graph)
            """
    def __init__(self):
        super().__init__()
        self.n_wires = 4
        self.q_device = tq.QuantumDevice(n_wires=self.n_wires)
        self.encoder = tq.GeneralEncoder(tq.encoder_op_list_name_dict['4_ry'])
        self.q_layer = self.QLayer()
        self.measure = tq.MeasureAll(tq.PauliZ)

    def forward(self, x, use_qiskit=False):
        # bsz = x.shape[0]
        # x = F.avg_pool2d(x, 6).view(bsz, 16)

        if use_qiskit:
            x = self.qiskit_processor.process_parameterized(self.q_device, self.encoder, self.q_layer, self.measure, x)
        else:
            self.encoder(self.q_device, x)
            self.q_layer(self.q_device)
            x = self.measure(self.q_device)

        # x = x.reshape(bsz, 2, 2).sum(-1).squeeze()
        x = F.softmax(x*4, dim=1)

        return x
    

class QCritic(tq.QuantumModule):
    class QLayer(tq.QuantumModule):
        def __init__(self):
            super().__init__()
            self.n_wires = 4
            self.random_layer = tq.RandomLayer(n_ops=50,
                                               wires=list(range(self.n_wires)))
            # gates with trainable parameters
            self.rx0 = tq.RX(has_params=True, trainable=True)
            self.ry0 = tq.RY(has_params=True, trainable=True)
            self.rz0 = tq.RZ(has_params=True, trainable=True)
            self.crx0 = tq.CRX(has_params=True, trainable=True)

        @tq.static_support
        def forward(self, q_device: tq.QuantumDevice):
            """
            1. To convert tq QuantumModule to qiskit or run in the static
            model, need to:
                (1) add @tq.static_support before the forward
                (2) make sure to add
                    static=self.static_mode and
                    parent_graph=self.graph
                    to all the tqf functions, such as tqf.hadamard below
            """
            self.q_device = q_device

            self.random_layer(self.q_device)

            # some trainable gates (instantiated ahead of time)
            self.rx0(self.q_device, wires=0)
            self.ry0(self.q_device, wires=1)
            self.rz0(self.q_device, wires=3)
            self.crx0(self.q_device, wires=[0, 2])
            """
            # add some more non-parameterized gates (add on-the-fly)
            tqf.hadamard(self.q_device, wires=3, static=self.static_mode,
                         parent_graph=self.graph)
            tqf.sx(self.q_device, wires=2, static=self.static_mode,
                   parent_graph=self.graph)
            tqf.cnot(self.q_device, wires=[3, 0], static=self.static_mode,
                     parent_graph=self.graph)
            """
    def __init__(self):
        super().__init__()
        self.n_wires = 4
        self.q_device = tq.QuantumDevice(n_wires=self.n_wires)
        self.encoder = tq.GeneralEncoder(tq.encoder_op_list_name_dict['4x4_ryzxy'])
        self.q_layer = self.QLayer()
        self.measure = tq.MeasureAll(tq.PauliZ)

    def forward(self, x, use_qiskit=False):
        # bsz = x.shape[0]
        # x = F.avg_pool2d(x, 6).view(bsz, 16)

        if use_qiskit:
            x = self.qiskit_processor.process_parameterized(self.q_device, self.encoder, self.q_layer, self.measure, x)
        else:
            self.encoder(self.q_device, x)
            self.q_layer(self.q_device)
            x = self.measure(self.q_device)
        x = x.sum(-1) * 20
        return x
    
class ReplayBuffer:
    def __init__(self,device):
        self.data = []
        self.device = device
    def put_data(self, transition):
        self.data.append(transition)
        
    def make_batch(self):
        s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []
        for transition in self.data:
            s, a, r, s_prime = transition
            s_lst.append(s)
            a_lst.append([a])
            r_lst.append([r])
            s_prime_lst.append(s_prime)
            
        s,a,r,s_prime = torch.tensor(s_lst, dtype=torch.float).to(self.device), torch.tensor(a_lst, dtype=torch.int64).to(self.device), \
                        torch.tensor(r_lst, dtype=torch.float).to(self.device), torch.tensor(s_prime_lst, dtype=torch.float).to(self.device)
        self.data = []
        return s, a, r, s_prime
        





import numpy as np
import copy
import torch
import torch.nn.functional as F
import torch.optim as optim
import argparse
import torch.nn as nn
import torchquantum as tq
import torchquantum.functional as tqf
from torch.optim.lr_scheduler import CosineAnnealingLR
from  torch.distributions import Categorical
from Environment import ENV
from network import QActor, QCritic,ReplayBuffer
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime

n_agents = 4 
s_dim = 4 
a_dim = 4 
gamma = 0.99
n_epochs = 1000
wires_per_block = 2 
a_lr = 1e-3
c_lr = 1e-3
static = 'store_true'

def train_actor(Experience, Pi, td_error, device, optimizer):
    S, A, R, S_Prime = Experience
    for i in range(n_agents):
        log_dist      = torch.log(Pi[i](S[:,i]))
        log_pi_a      = log_dist.gather(-1, A[:,:,i])
        loss          = - (td_error.detach() * log_pi_a).sum()
        
        optimizer[i].zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(Pi[i].parameters(), 10)
        optimizer[i].step()
        
def train_critic(t, Experience, V, VTarget, optimizer):
    V.train()
    VTarget.eval()
    S, A, R, S_Prime = Experience
    State       = S.reshape(S.shape[0], -1)
    State_Prime = S_Prime.reshape(S_Prime.shape[0], -1)
    R = R.sum(-1)
    v             = V(State)
    vtarget       = VTarget(State_Prime)
    targets       = R + gamma * vtarget 
    td_error      = targets.detach() - v
    loss          = (td_error ** 2).sum()
    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(V.parameters(), 10)
    optimizer.step()
    if t > 0 and t % 10 == 0:
        VTarget.load_state_dict(V.state_dict())
    return td_error
        
def train():    
    env  = ENV()
    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")
    actors = [QActor().to(device) for _ in range(n_agents)]
    critic = QCritic().to(device)    
    critic_target = copy.deepcopy(critic)
    critic_target.load_state_dict(critic.state_dict())
    replay     = ReplayBuffer(device)
    Qoptimizer = [optim.Adam(actors[i].parameters(), lr=a_lr) for i in range(n_agents)]
    Scheduler  = [CosineAnnealingLR(Qoptimizer[i], T_max=n_epochs) for i in range(n_agents)]
    Voptimizer = optim.Adam(critic.parameters(), lr = c_lr)

    if static:
        for i in range(n_agents):
            
            actors[i].q_layer.static_on(wires_per_block=wires_per_block)

    for epoch in range(1, n_epochs + 1):
        
        s = env.reset()
        s_prime = np.copy(s)
        score = 0
        for t in range(env.T_MAX):
            
            s = s_prime.copy()
            a = []
            for i in range(n_agents):
                action_dist  = actors[i](torch.from_numpy(s[i]).unsqueeze(0).to(device, dtype=torch.float))
                a.append(Categorical(action_dist).sample().item())
            a = np.array(a)
            r, s_prime, done = env.step(a)
            transition = [s,a,r,s_prime]
            replay.put_data(transition)
            score += r
        print(f'Epoch {epoch}/ Reward\t', score)
            
        experience = replay.make_batch()
        td_error = train_critic(t=epoch, 
                                Experience= experience, 
                                V = critic, 
                                VTarget= critic_target, 
                                optimizer= Voptimizer)
        
        train_actor(Experience=experience, 
                    Pi= actors, 
                    td_error= td_error, 
                    device= device, 
                    optimizer= Qoptimizer)
        
        for i in range(n_agents):
            Scheduler[i].step()
            
        for i in range(n_agents):
            torch.save(actors[i].state_dict(),'./Qagent.pkl')
        torch.save(critic.state_dict()       ,'./Qcritic.pkl')
            
if __name__ == "__main__":
    train()
